{"cells":[{"cell_type":"code","source":["# # Reddit Comments Analysis\n# ### Download and clean data. Estimate polarity scores.\n\n# Reddit monthly comments are zipped and available for some modeling.\n# URL for this is as following https://files.pushshift.io/reddit/comments/.\n# Since databricks environment is limited to 10GB, only smaller files are downloaded.\n\n# First, I download 2011 September comments."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["## nltk is required for sentiment analysis\n!pip install nltk"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["## download important libs\n\nfrom pyspark import SparkContext\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport json\nimport bz2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# nltk.download('punkt')\n# nltk.download('stopwords')\nnltk.download('vader_lexicon')\n# nltk.download('wordnet')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\nOut[21]: True</div>"]}}],"execution_count":4},{"cell_type":"code","source":["## this will save zipped file in temp folder\n!wget 'https://files.pushshift.io/reddit/comments/RC_2011-09.bz2'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["## import bz2 and unzip the file\nwith bz2.open(\"RC_2011-09.bz2\", \"rb\") as f:\n    content = f.read()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["## make txt file and write the content\nf = open(\"RC_comment_09.txt\", \"wb\")\nf.write(content)\nf.close()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["## move files from driver to dbfs file storage\n## data is moved to databrick's local storage for further processing\n\ndbutils.fs.mv(\"file:/databricks/driver/RC_comment_09.txt\", \n              \"dbfs:/tmp/RC_2011-09.txt\")  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1977860122152771&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> <span class=\"ansi-red-fg\"># dbutils.fs.mv(&#34;file:/databricks/driver/RC_2011-08.bz2&#34;, &#34;dbfs:/tmp/RC_2011-08.bz2&#34;)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span> <span class=\"ansi-red-fg\"># ## move files from driver to dbfs file storage</span>\n<span class=\"ansi-green-fg\">----&gt; 4</span><span class=\"ansi-red-fg\"> </span>dbutils<span class=\"ansi-blue-fg\">.</span>fs<span class=\"ansi-blue-fg\">.</span>mv<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;file:/databricks/driver/RC_comment10.txt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;dbfs:/tmp/RC_2011-10.txt&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/local_disk0/tmp/1598095973346-0/dbutils.py</span> in <span class=\"ansi-cyan-fg\">f_with_exception_handling</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    312</span>                     exc<span class=\"ansi-blue-fg\">.</span>__context__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    313</span>                     exc<span class=\"ansi-blue-fg\">.</span>__cause__ <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-fg\">--&gt; 314</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">raise</span> exc\n<span class=\"ansi-green-intense-fg ansi-bold\">    315</span>             <span class=\"ansi-green-fg\">return</span> f_with_exception_handling\n<span class=\"ansi-green-intense-fg ansi-bold\">    316</span> \n\n<span class=\"ansi-red-fg\">ExecutionError</span>: An error occurred while calling z:com.databricks.backend.daemon.dbutils.FSUtils.mv.\n: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum total size of files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Bytes allocated: 15671 MB); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum total size of files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Bytes allocated: 15671 MB)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:123)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:83)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:62)\n\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:120)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.cpRecursive(DBUtilsCore.scala:185)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$mv$2(DBUtilsCore.scala:157)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$mv$1(DBUtilsCore.scala:134)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:81)\n\tat com.databricks.backend.daemon.dbutils.FSUtils$.mv(DBUtilsCore.scala:134)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.mv(DBUtilsCore.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum total size of files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Bytes allocated: 15671 MB)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:317)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:311)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:308)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:307)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:541)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:539)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:145)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:548)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:36)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:161)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:238)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:233)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:230)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:23)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:275)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:268)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:23)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:147)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:98)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:83)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:62)\n\t\tat org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:120)\n\t\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:366)\n\t\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)\n\t\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils$.cpRecursive(DBUtilsCore.scala:185)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$mv$2(DBUtilsCore.scala:157)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:81)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils$.$anonfun$mv$1(DBUtilsCore.scala:134)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils$.withFsSafetyCheck(DBUtilsCore.scala:81)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils$.mv(DBUtilsCore.scala:134)\n\t\tat com.databricks.backend.daemon.dbutils.FSUtils.mv(DBUtilsCore.scala)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\t\tat py4j.Gateway.invoke(Gateway.java:295)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\t\t... 1 more\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["## create rdd to further work on it\nrdd = sc.textFile(\"dbfs:/tmp/RC_2011-09.txt\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["## data is list of dictionaries\nprint(rdd.take(1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;{&#34;subreddit&#34;:&#34;woahdude&#34;,&#34;subreddit_id&#34;:&#34;t5_2r8tu&#34;,&#34;score_hidden&#34;:false,&#34;edited&#34;:true,&#34;author&#34;:&#34;TheBeardlessSquirrel&#34;,&#34;controversiality&#34;:0,&#34;retrieved_on&#34;:1427543912,&#34;author_flair_text&#34;:null,&#34;name&#34;:&#34;t1_c2gmvge&#34;,&#34;gilded&#34;:0,&#34;link_id&#34;:&#34;t3_k05iw&#34;,&#34;created_utc&#34;:&#34;1314835200&#34;,&#34;author_flair_css_class&#34;:null,&#34;score&#34;:1,&#34;distinguished&#34;:null,&#34;parent_id&#34;:&#34;t3_k05iw&#34;,&#34;downs&#34;:0,&#34;id&#34;:&#34;c2gmvge&#34;,&#34;body&#34;:&#34;At least wait a bit before [reposting](http://www.reddit.com/r/woahdude/comments/jyxly/mighty_morphing_power_art_gif/).&#34;,&#34;ups&#34;:1,&#34;archived&#34;:true}&#39;]\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["## Here I start taking a subset"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["## txt file shows that it is txt of json \n## keep only unix time information and comments\n## also data that has 'deleted' info has been removed\n\n## rdd keys are comment ids give under \"name\"\nrdd_subset = rdd.map(lambda line : (json.loads(line)['name'],                                    \n                                    json.loads(line)['author'],\n                                    json.loads(line)['author_flair_text'],\n                                    json.loads(line)['created_utc'],\n                                    json.loads(line)['parent_id'],\n                                    json.loads(line)['ups'],\n                                    json.loads(line)['downs'],\n                                    json.loads(line)['retrieved_on'],\n                                    json.loads(line)['subreddit'],\n                                    json.loads(line)['body'],)\n                    ).filter(lambda line: line if '[deleted]' not in line[9] else None)\n\ndf = spark.createDataFrame(rdd_subset).toDF(\"name\",\"author\",\n                                                       \"author_flair_text\",\n                                                       \"unix_time\",\"parent_id\",\n                                                       \"ups\",\"downs\",\"retrieved_on\",\n                                                       \"subreddit\", \"comment\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["## first i loop through comments and find average len of words in every comment\nrdd_commments = rdd_subset.map(lambda line: line[-1])\nrdd_commments.cache()\n\nrdd_comments_ave_len_words = rdd_commments.map(lambda line: \n                  sum([len(part) for part in line.split(' ')]) / len(line.split(' ')) \n                 )\n\n\nave_string_mean = rdd_comments_ave_len_words.mean()\nave_string_std = rdd_comments_ave_len_words.sampleStdev()\n\n## second i loop through comments and find longest len of words\n## this will help to know the len of long strings and then I remove \n## long strings that do not have any semantic value\n## such as urls etc.\nrdd_comments_longest_len_words = rdd_commments.map(lambda line: \n                  max( \n                    [len(part) for part in line.split(' ')]\n                  ))\nlong_string_mean = rdd_comments_longest_len_words.mean()\nlong_string_std = rdd_comments_longest_len_words.sampleStdev()\n\n## third loop through and see if comments just keep only numbers or just None\nrdd_comments_none = rdd_commments.map(lambda line: True if type(line) is type(None) else False)\nrdd_comments_int = rdd_commments.map(lambda line: True if type(line) is type(int()) else False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["## average string len \n## this looks a little bit longer than 4.7 char\n## as explained in this link below, English words on average has 4.7 char\n## http://norvig.com/mayzner.html\n## so I will remove very long strings \nave_string_mean"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[22]: 5.500056123492271</div>"]}}],"execution_count":14},{"cell_type":"code","source":["ave_string_std"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: 13.457259730113545</div>"]}}],"execution_count":15},{"cell_type":"code","source":["## Long strings are too long so their average as well\n## i use this 14 as a cutoff length to trim tokens later on in short_words function below\nlong_string_mean"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[91]: 14.468464958205868</div>"]}}],"execution_count":16},{"cell_type":"code","source":["long_string_std"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[44]: 26.213877524345452</div>"]}}],"execution_count":17},{"cell_type":"code","source":["## all values are false\n## so there is no comment that is only int\nany(rdd_comments_int.collect())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[73]: False</div>"]}}],"execution_count":18},{"cell_type":"code","source":["## all values are false\n## so there is no comment that is only None\nany(rdd_comments_none.collect())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[69]: False</div>"]}}],"execution_count":19},{"cell_type":"code","source":["## now i go back to df and start shortening and tokenizing comments\n## subset looks fine\n## it has unix time, comments and all other info that is needed\nprint(df.show(1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+-----------------+----------+---------+---+-----+------------+---------+--------------------+\n      name|              author|author_flair_text| unix_time|parent_id|ups|downs|retrieved_on|subreddit|             comment|\n+----------+--------------------+-----------------+----------+---------+---+-----+------------+---------+--------------------+\nt1_c2gmvge|TheBeardlessSquirrel|             null|1314835200| t3_k05iw|  1|    0|  1427543912| woahdude|At least wait a b...|\n+----------+--------------------+-----------------+----------+---------+---+-----+------------+---------+--------------------+\nonly showing top 1 row\n\nNone\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["## here I create several functions for processing\n  \nfrom pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\nfrom pyspark.sql.functions import udf\nfrom pyspark.ml.feature import Tokenizer\nfrom pyspark.ml.feature import RegexTokenizer\nanalyzer = SentimentIntensityAnalyzer()\n\ndef remove_int(x):\n    return [a for a in x if a.isdigit() is False]\n\ndef short_words(x):\n    return [a for a in x if len(a) <= 14]\n    \ndef join_tokens(x):\n    return [\" \".join(x)]\n  \ndef sentiment_score(x):\n    vs = analyzer.polarity_scores(x[0])\n    return vs['neg'], vs['neu'], vs['pos'], vs['compound']\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["regtokenizer = RegexTokenizer(inputCol='comment', outputCol='comment_tokens', toLowercase = False)\ndf_regtokened = regtokenizer.transform(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["remove_int_udf = udf(lambda line: remove_int(line), ArrayType(StringType()))\nshorten_words_udf = udf(lambda line: short_words(line), ArrayType(StringType()))\njoin_tokens_udf = udf(lambda line: join_tokens(line), ArrayType(StringType()))\nsentiment_score_udf = udf(lambda line: sentiment_score(line), ArrayType(StringType()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["df_tokened = df_regtokened.withColumn('comment_tokens_cleaned', remove_int_udf(\"comment_tokens\"))\n\ndf_tokened = df_tokened.withColumn('comment_tokens_cleaned', shorten_words_udf(\"comment_tokens_cleaned\"))\ndf_tokened = df_tokened.withColumn('comment_tokens_joined', join_tokens_udf(\"comment_tokens_cleaned\"))\n\ndf_tokened = df_tokened.withColumn('comment_sentiment', sentiment_score_udf(\"comment_tokens_joined\"))\ndf_tokened = df_tokened.withColumn('sentiment_neg', df_tokened.comment_sentiment[0]\n                                  ).withColumn('sentiment_neu', df_tokened.comment_sentiment[1]\n                                  ).withColumn('sentiment_pos', df_tokened.comment_sentiment[2]\n                                  ).withColumn('sentiment_com', df_tokened.comment_sentiment[3])\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["df_tokened.select('comment_sentiment', 'sentiment_neg', 'sentiment_neu','sentiment_pos', 'sentiment_com').show(10, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+-------------+-------------+-------------+-------------+\ncomment_sentiment            |sentiment_neg|sentiment_neu|sentiment_pos|sentiment_com|\n+-----------------------------+-------------+-------------+-------------+-------------+\n[0.0, 1.0, 0.0, 0.0]         |0.0          |1.0          |0.0          |0.0          |\n[0.0, 1.0, 0.0, 0.0]         |0.0          |1.0          |0.0          |0.0          |\n[0.093, 0.809, 0.098, 0.0516]|0.093        |0.809        |0.098        |0.0516       |\n[0.0, 1.0, 0.0, 0.0]         |0.0          |1.0          |0.0          |0.0          |\n[0.0, 1.0, 0.0, 0.0]         |0.0          |1.0          |0.0          |0.0          |\n[0.0, 0.649, 0.351, 0.6597]  |0.0          |0.649        |0.351        |0.6597       |\n[0.0, 0.672, 0.328, 0.9442]  |0.0          |0.672        |0.328        |0.9442       |\n[0.0, 1.0, 0.0, 0.0]         |0.0          |1.0          |0.0          |0.0          |\n[0.084, 0.805, 0.111, 0.1779]|0.084        |0.805        |0.111        |0.1779       |\n[0.0, 0.915, 0.085, 0.0772]  |0.0          |0.915        |0.085        |0.0772       |\n+-----------------------------+-------------+-------------+-------------+-------------+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.types import FloatType\n\ndef find_average_len_words(line):\n    return sum([len(part) for part in line.split(' ')]) / len(line.split(' '))\n  \nfind_average_len_words_udf = udf(lambda line: find_average_len_words(line[0]), FloatType())\n\ndf_tokened_check = df_tokened.withColumn('comment_tokens_mean', find_average_len_words_udf(\"comment_tokens_joined\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql.functions import mean, stddev, col\n\ndf_tokened_check.select(mean(col('comment_tokens_mean')), stddev(col('comment_tokens_mean'))).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------+--------------------------------+\navg(comment_tokens_mean)|stddev_samp(comment_tokens_mean)|\n+------------------------+--------------------------------+\n        4.52885701992511|              1.0821986118738078|\n+------------------------+--------------------------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["## now that avg comment token is close to 4.7\n## http://norvig.com/mayzner.html\n\n## also stdev is reduced considerebly"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# VADER relies on several key words in the sentence\n# removing them would alter the polarity scores\n# - conjuctions (no stopword removal)\n# - degree modifiers (no lemmatizing)\n# - capitalization (no lowercasing)\n# - punctuation (no punctuation removal)\n\n# Still I will have functions for each case that could be used later on"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["# Here I do a simple check.\n# To see how urls in the string will effect scores.\n# Urls and long strings wont effect. They will be removed to keep our data in smaller size."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["print(analyzer.polarity_scores(\"at Least wait a bit before bad [reposting]\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;neg&#39;: 0.368, &#39;neu&#39;: 0.632, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["print(analyzer.polarity_scores(\"at least wait a bit before bad [reposting](http://www.reddit.com/r/woahdude/comments/jyxly/mighty_morphing_power_art_gif/).\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;neg&#39;: 0.414, &#39;neu&#39;: 0.586, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.6408}\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# Now I check how numbers alter polarity scores.\n# Numbers affect our scores. But numbers are not useful since they have no semantic value. They will be removed. "],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["print(analyzer.polarity_scores(\"at least wait a bit before bad [reposting]\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;neg&#39;: 0.368, &#39;neu&#39;: 0.632, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["print(analyzer.polarity_scores(\"at Least wait a bit before bad [reposting] 123\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;neg&#39;: 0.333, &#39;neu&#39;: 0.667, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["# So only removed \n# - very long words\n# - numbers "],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# Data in rdd are just rows.\n# Each function that is applied on rdd goes through each row.\n# In rdd line[0] is unix time and line[1] is comments in string.\n# Functions are applied by lambda and only uses line[1] since it contains comments."],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# ## tokenize for sentences\n# ## rdd rows are given in the tuple\n# ## rdd_subset contains comments in the last part of the tuple\n# ## so I return all elements until the last one and modify the last element which is reddit comment\n# sent_rdd = rdd_subset.map(lambda line: (line[:9], \n#                                         tokenize_sent(line[9])))\n\n# ## tokenize for words\n# ## now sent_rdd is changed and each row is tuple of tuple and list together\n# ## tuple's second element is list which is modified comments\n# ## tuple's first element is tuple of line[:9] from the first step\n# word_rdd = sent_rdd.map(lambda line: (line[0], \n#                                       tokenize_word(line[1])))\n\n# ## remove int\n# removed_int_rdd = word_rdd.map(lambda line: (line[0], \n#                                              remove_int(line[1])))\n\n# ## remove long tokens\n# shortened_rdd = removed_int_rdd.map(lambda line: (line[0], \n#                                                   short_words(line[1])))\n\n# ## join cleaned tokens for sentiment analysis\n# joined_rdd = shortened_rdd.map(lambda line: (line[0], \n#                                              join_tokens(line[1])))\n\n# ## sentiments are added as well \n# sentiment_rdd = joined_rdd.map(lambda line: (line[0], line[1], \n#                                              sentiment_score(line[1])))\n\n# ## sentiment scores are wrapped in tuples\n# ## so now each row is tuple + list + tuple\n# ## here i open last tuple\n# rdd_processed = sentiment_rdd.map(lambda line: (line[0], line[1][0], \n#                                                 line[2][0], line[2][1], \n#                                                 line[2][2], line[2][3]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":38},{"cell_type":"code","source":["# ## here I just open nested tuples for each row/line tuple \n# rdd_processed_ = rdd_processed.map(lambda line: (line[0][0], line[0][1], line[0][2],\n#                                                  line[0][3], line[0][4], line[0][5],\n#                                                  line[0][6], line[0][7], line[0][8], \n#                                                  line[1], line[2], line[3], line[4], line[5]))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"code","source":["# ## now our data is as following, just list\n# print(rdd_processed_.take(1))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[(&#39;t1_c2gmvge&#39;, &#39;TheBeardlessSquirrel&#39;, None, &#39;1314835200&#39;, &#39;t3_k05iw&#39;, 1, 0, 1427543912, &#39;woahdude&#39;, &#39;At least wait a bit before&#39;, 0.0, 1.0, 0.0, 0.0)]\n</div>"]}}],"execution_count":40},{"cell_type":"code","source":["# ## here I create a dataframe from rdd and give column names\n# df_subset = spark.createDataFrame(rdd_processed_).toDF(\"name\",\"author\",\n#                                                        \"author_flair_text\",\n#                                                        \"unix_time\",\"parent_id\",\n#                                                        \"ups\",\"downs\",\"retrieved_on\",\n#                                                        \"subreddit\", \"comment\", \"neg\", \n#                                                        \"neu\", \"pos\", \"com\")\n# # print(df_subset.show(1, truncate=False))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["# display(df_subset.show(1, truncate=False))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"code","source":["## save the df\ndf_subset.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"dbfs:/FileStore/tmp/df_09_subset.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":43},{"cell_type":"code","source":["## check if the data is saved\n## it will be used by another notebook \n%fs ls dbfs:/FileStore/tmp"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tmp/df_09_subset.csv/</td><td>df_09_subset.csv/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":44}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"2.2 Working with Text Files","notebookId":1977860122152764},"nbformat":4,"nbformat_minor":0}
