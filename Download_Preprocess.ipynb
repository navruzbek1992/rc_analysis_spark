{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Comments Analysis\n",
    "\n",
    "### Download and clean data. Estimate polarity scores.\n",
    "\n",
    "Reddit monthly comments are zipped and available for some modeling.\n",
    "\n",
    "URL for this is as following https://files.pushshift.io/reddit/comments/.\n",
    "\n",
    "Since databricks environment is limited to 10GB, only smaller files are download.\n",
    "\n",
    "First, I download 2011 September comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## nltk is required for sentiment analysis\n",
    "!pip install nltk\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## download important libs\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud \n",
    "import json\n",
    "import bz2\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[nltk_data] Downloading package punkt to /root/nltk_data...\n",
       "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
       "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
       "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
       "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
       "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
       "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
       "Out[7]: True</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## this will save zipped file in temp folder\n",
    "!wget 'https://files.pushshift.io/reddit/comments/RC_2011-09.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## import bz2 and unzip the file\n",
    "with bz2.open(\"RC_2011-09.bz2\", \"rb\") as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## make txt file and write the content\n",
    "f = open(\"RC_comment_09.txt\", \"wb\")\n",
    "f.write(content)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## move files from driver to dbfs file storage\n",
    "## data is moved to databrick's local storage for further processing\n",
    "\n",
    "dbutils.fs.mv(\"file:/databricks/driver/RC_comment_09.txt\", \n",
    "              \"dbfs:/tmp/RC_2011-09.txt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create rdd to further work on it\n",
    "rdd = sc.textFile(\"dbfs:/tmp/RC_2011-09.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;{&#34;subreddit&#34;:&#34;woahdude&#34;,&#34;subreddit_id&#34;:&#34;t5_2r8tu&#34;,&#34;score_hidden&#34;:false,&#34;edited&#34;:true,&#34;author&#34;:&#34;TheBeardlessSquirrel&#34;,&#34;controversiality&#34;:0,&#34;retrieved_on&#34;:1427543912,&#34;author_flair_text&#34;:null,&#34;name&#34;:&#34;t1_c2gmvge&#34;,&#34;gilded&#34;:0,&#34;link_id&#34;:&#34;t3_k05iw&#34;,&#34;created_utc&#34;:&#34;1314835200&#34;,&#34;author_flair_css_class&#34;:null,&#34;score&#34;:1,&#34;distinguished&#34;:null,&#34;parent_id&#34;:&#34;t3_k05iw&#34;,&#34;downs&#34;:0,&#34;id&#34;:&#34;c2gmvge&#34;,&#34;body&#34;:&#34;At least wait a bit before [reposting](http://www.reddit.com/r/woahdude/comments/jyxly/mighty_morphing_power_art_gif/).&#34;,&#34;ups&#34;:1,&#34;archived&#34;:true}&#39;]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## data is list of dictionaries\n",
    "print(rdd.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I start taking a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## txt file shows that it is txt of json \n",
    "## keep only unix time information and comments\n",
    "## also data that has 'deleted' info has been removed\n",
    "\n",
    "## rdd keys are comment ids give under \"name\"\n",
    "rdd_subset = rdd.map(lambda line : (json.loads(line)['name'],                                    \n",
    "                                    json.loads(line)['author'],\n",
    "                                    json.loads(line)['author_flair_text'],\n",
    "                                    json.loads(line)['created_utc'],\n",
    "                                    json.loads(line)['parent_id'],\n",
    "                                    json.loads(line)['ups'],\n",
    "                                    json.loads(line)['downs'],\n",
    "                                    json.loads(line)['retrieved_on'],\n",
    "                                    json.loads(line)['subreddit'],\n",
    "                                    json.loads(line)['body'],)\n",
    "                    ).filter(lambda line: line if '[deleted]' not in line[9] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[(&#39;t1_c2gmvge&#39;, &#39;TheBeardlessSquirrel&#39;, None, &#39;1314835200&#39;, &#39;t3_k05iw&#39;, 1, 0, 1427543912, &#39;woahdude&#39;, &#39;At least wait a bit before [reposting](http://www.reddit.com/r/woahdude/comments/jyxly/mighty_morphing_power_art_gif/).&#39;)]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## subset looks fine\n",
    "## it has unix time, comments and all other info that is needed\n",
    "print(rdd_subset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## here I create several functions for processing\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def tokenize_sent(x):\n",
    "    return nltk.sent_tokenize(x)\n",
    "  \n",
    "def tokenize_word(x):\n",
    "    sent_split = [word for line in x for word in line.split()]\n",
    "    return sent_split\n",
    "  \n",
    "## is not int\n",
    "def remove_int(x):\n",
    "    not_int = [a for a in x if a.isdigit() is False]\n",
    "    return not_int\n",
    "  \n",
    "def short_words(x):\n",
    "    short = [a for a in x if len(a) <= 20]\n",
    "    return short\n",
    "  \n",
    "## not needed for vader\n",
    "stopwords_en=set(stopwords.words('english'))\n",
    "def remove_stopwords(x):\n",
    "    cleaned_sent = [w for w in x if not w in stopwords_en]\n",
    "    return cleaned_sent\n",
    "  \n",
    "## not needed for vader\n",
    "punct_words=list(string.punctuation)\n",
    "def remove_punct(x):\n",
    "    cleaned_sent = [''.join(c for c in s if c not in punct_words) for s in x] \n",
    "    cleaned_sent = [s for s in cleaned_sent if s] #remove empty space \n",
    "    return cleaned_sent\n",
    "\n",
    "## not needed for vader\n",
    "def lemmatize_sent(x):\n",
    "    lemma = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return lemma\n",
    "  \n",
    "def join_tokens(x):\n",
    "    x = \" \".join(x)\n",
    "    return [x]\n",
    "  \n",
    "def sentiment_score(x):\n",
    "    vs = analyzer.polarity_scores(x[0])\n",
    "    return vs['neg'], vs['neu'], vs['pos'], vs['compound']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER relies on several key words in the sentence\n",
    "removing them would alter the polarity scores\n",
    "- conjuctions (no stopword removal)\n",
    "- degree modifiers (no lemmatizing)\n",
    "- capitalization (no lowercasing)\n",
    "- punctuation (no punctuation removal)\n",
    "\n",
    "Still I will have functions for each case that could be used later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do a simple check.\n",
    "To see how urls in the string will effect scores.\n",
    "\n",
    "Urls and long strings wont effect. They will be removed to keep our data in smaller size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&#39;neg&#39;: 0.368, &#39;neu&#39;: 0.632, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(analyzer.polarity_scores(\"at Least wait a bit before bad [reposting]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&#39;neg&#39;: 0.368, &#39;neu&#39;: 0.632, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(analyzer.polarity_scores(\"at least wait a bit before bad [reposting](http://www.reddit.com/r/woahdude/comments/jyxly/mighty_morphing_power_art_gif/).\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I check how numbers alter polarity scores.\n",
    "\n",
    "Numbers affect our scores. But numbers are not useful since they have no semantic value. They will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&#39;neg&#39;: 0.368, &#39;neu&#39;: 0.632, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(analyzer.polarity_scores(\"at least wait a bit before bad [reposting]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&#39;neg&#39;: 0.333, &#39;neu&#39;: 0.667, &#39;pos&#39;: 0.0, &#39;compound&#39;: -0.5423}\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(analyzer.polarity_scores(\"at Least wait a bit before bad [reposting] 123\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So only removed \n",
    "- very long words\n",
    "- numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in rdd are just rows.\n",
    "\n",
    "Each function that is applied on rdd goes through each row.\n",
    "\n",
    "In rdd line[0] is unix time and line[1] is comments in string.\n",
    "\n",
    "Functions are applied by lambda and only uses line[1] since it contains comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## tokenize for sentences\n",
    "## rdd rows are given in the tuple\n",
    "## rdd_subset contains comments in the last part of the tuple\n",
    "## so I return all elements until the last one and modify the last element which is reddit comment\n",
    "sent_rdd = rdd_subset.map(lambda line: (line[:9], \n",
    "                                        tokenize_sent(line[9])))\n",
    "\n",
    "## tokenize for words\n",
    "## now sent_rdd is changed and each row is tuple of tuple and list together\n",
    "## tuple's second element is list which is modified comments\n",
    "## tuple's first element is tuple of line[:9] from the first step\n",
    "word_rdd = sent_rdd.map(lambda line: (line[0], \n",
    "                                      tokenize_word(line[1])))\n",
    "\n",
    "## remove int\n",
    "removed_int_rdd = word_rdd.map(lambda line: (line[0], \n",
    "                                             remove_int(line[1])))\n",
    "\n",
    "## remove long tokens\n",
    "shortened_rdd = removed_int_rdd.map(lambda line: (line[0], \n",
    "                                                  short_words(line[1])))\n",
    "\n",
    "## join cleaned tokens for sentiment analysis\n",
    "joined_rdd = shortened_rdd.map(lambda line: (line[0], \n",
    "                                             join_tokens(line[1])))\n",
    "\n",
    "## sentiments are added as well \n",
    "sentiment_rdd = joined_rdd.map(lambda line: (line[0], line[1], \n",
    "                                             sentiment_score(line[1])))\n",
    "\n",
    "## sentiment scores are wrapped in tuples\n",
    "## so now each row is tuple + list + tuple\n",
    "## here i open last tuple\n",
    "rdd_processed = sentiment_rdd.map(lambda line: (line[0], line[1][0], \n",
    "                                                line[2][0], line[2][1], \n",
    "                                                line[2][2], line[2][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## here I just open nested tuples for each row/line tuple \n",
    "rdd_processed_ = rdd_processed.map(lambda line: (line[0][0], line[0][1], line[0][2],\n",
    "                                                 line[0][3], line[0][4], line[0][5],\n",
    "                                                 line[0][6], line[0][7], line[0][8], \n",
    "                                                 line[1], line[2], line[3], line[4], line[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[(&#39;t1_c2gmvge&#39;, &#39;TheBeardlessSquirrel&#39;, None, &#39;1314835200&#39;, &#39;t3_k05iw&#39;, 1, 0, 1427543912, &#39;woahdude&#39;, &#39;At least wait a bit before&#39;, 0.0, 1.0, 0.0, 0.0)]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## now our data is as following, just list\n",
    "print(rdd_processed_.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## here I create a dataframe from rdd and give column names\n",
    "df_subset = spark.createDataFrame(rdd_processed_).toDF(\"name\",\"author\",\n",
    "                                                       \"author_flair_text\",\n",
    "                                                       \"unix_time\",\"parent_id\",\n",
    "                                                       \"ups\",\"downs\",\"retrieved_on\",\n",
    "                                                       \"subreddit\", \"comment\", \"neg\", \n",
    "                                                       \"neu\", \"pos\", \"com\")\n",
    "# print(df_subset.show(1, truncate=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------+--------------------+-----------------+----------+---------+---+-----+------------+---------+--------------------------+---+---+---+---+\n",
       "name      |author              |author_flair_text|unix_time |parent_id|ups|downs|retrieved_on|subreddit|comment                   |neg|neu|pos|com|\n",
       "+----------+--------------------+-----------------+----------+---------+---+-----+------------+---------+--------------------------+---+---+---+---+\n",
       "t1_c2gmvge|TheBeardlessSquirrel|null             |1314835200|t3_k05iw |1  |0    |1427543912  |woahdude |At least wait a bit before|0.0|1.0|0.0|0.0|\n",
       "+----------+--------------------+-----------------+----------+---------+---+-----+------------+---------+--------------------------+---+---+---+---+\n",
       "only showing top 1 row\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_subset.show(1, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## save the df\n",
    "df_subset.write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"dbfs:/FileStore/tmp/df_09_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## check if the data is saved\n",
    "## it will be used by another notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tmp/df_09_subset.csv/</td><td>df_09_subset.csv/</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/FileStore/tmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "2.2 Working with Text Files",
  "notebookId": 1977860122152764
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
