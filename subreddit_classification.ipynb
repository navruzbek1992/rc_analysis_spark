{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "from pyspark.sql.functions import udf, col, count\n",
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, Tokenizer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create rdd to further work on it\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rdd = sc.textFile(\"RC_comment_08.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"link_id\":\"t3_j4zx3\",\"score_hidden\":false,\"score\":1,\"archived\":true,\"author_flair_text\":null,\"subreddit\":\"fffffffuuuuuuuuuuuu\",\"body\":\"\\\\\"$2, would you take that deal? I\\'d take that deal\\\\\"\",\"author\":\"DorkyDude\",\"distinguished\":null,\"parent_id\":\"t3_j4zx3\",\"id\":\"c298mtc\",\"subreddit_id\":\"t5_2qqlo\",\"controversiality\":0,\"gilded\":0,\"downs\":0,\"retrieved_on\":1427415708,\"name\":\"t1_c298mtc\",\"ups\":1,\"edited\":false,\"author_flair_css_class\":null,\"created_utc\":\"1312156800\"}']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## txt file shows that it is txt of json \n",
    "## keep only unix time information and comments\n",
    "## also data that has 'deleted' info has been removed\n",
    "\n",
    "## rdd keys are comment ids give under \"name\"\n",
    "rdd_subset = rdd.map(lambda line : (json.loads(line)['name'],                                    \n",
    "                                    json.loads(line)['author'],\n",
    "                                    json.loads(line)['author_flair_text'],\n",
    "                                    json.loads(line)['created_utc'],\n",
    "                                    json.loads(line)['parent_id'],\n",
    "                                    json.loads(line)['ups'],\n",
    "                                    json.loads(line)['downs'],\n",
    "                                    json.loads(line)['retrieved_on'],\n",
    "                                    json.loads(line)['subreddit'],\n",
    "                                    json.loads(line)['body'],)\n",
    "                    ).filter(lambda line: line if '[deleted]' not in line[9] else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate() \n",
    "\n",
    "## here I create a dataframe from rdd and give column names\n",
    "df = spark.createDataFrame(rdd_subset).toDF(\"name\",\"author\",\n",
    "                                             \"author_flair_text\",\n",
    "                                             \"unix_time\",\"parent_id\",\n",
    "                                             \"ups\",\"downs\",\"retrieved_on\",\n",
    "                                             \"subreddit\", \"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+----------+----------+---+-----+------------+-------------------+--------------------+\n",
      "|      name|   author|author_flair_text| unix_time| parent_id|ups|downs|retrieved_on|          subreddit|             comment|\n",
      "+----------+---------+-----------------+----------+----------+---+-----+------------+-------------------+--------------------+\n",
      "|t1_c298mtc|DorkyDude|             null|1312156800|  t3_j4zx3|  1|    0|  1427415708|fffffffuuuuuuuuuuuu|\"$2, would you ta...|\n",
      "|t1_c298mtg|  TrptJim|             null|1312156800|t1_c293dct|  2|    0|  1427415708|        motorcycles|Have you wrecked ...|\n",
      "+----------+---------+-----------------+----------+----------+---+-----+------------+-------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, author: string, author_flair_text: string, unix_time: string, parent_id: string, ups: bigint, downs: bigint, retrieved_on: bigint, subreddit: string, comment: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here I check number of comments for each group\n",
    "count_by_subreddit = df.select('comment','subreddit').groupBy('subreddit').agg(count('comment'))\n",
    "count_by_subreddit_ = count_by_subreddit.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9411"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## so we have almost 10000 subreddits/classes in all data\n",
    "len(count_by_subreddit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|     subreddit|count(comment)|\n",
      "+--------------+--------------+\n",
      "|          NOLA|             1|\n",
      "|          entp|             1|\n",
      "|   testasextoy|             1|\n",
      "|     herobrine|             1|\n",
      "|      spelling|             1|\n",
      "|     deardiary|             1|\n",
      "|deletefacebook|             1|\n",
      "|     pantyhose|             1|\n",
      "|           WDP|             1|\n",
      "|  neanderthals|             1|\n",
      "+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "count_by_subreddit_df = spark.createDataFrame(count_by_subreddit_)\n",
    "print(count_by_subreddit_df.sort('count(comment)').show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|          subreddit|count(comment)|\n",
      "+-------------------+--------------+\n",
      "|          AskReddit|       1659075|\n",
      "|               pics|        888750|\n",
      "|         reddit.com|        497388|\n",
      "|             gaming|        496350|\n",
      "|              funny|        399773|\n",
      "|fffffffuuuuuuuuuuuu|        380435|\n",
      "|               IAmA|        364646|\n",
      "|           politics|        286835|\n",
      "|              trees|        260490|\n",
      "|            atheism|        240203|\n",
      "+-------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## here i just choose randomly two classes for classification\n",
    "print(count_by_subreddit_df.sort(col('count(comment)').desc()).show(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trees/gaming\n",
    "df_filtered = df.filter((col(\"subreddit\") == \"gaming\") | (col(\"subreddit\") == \"trees\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|subreddit|count(comment)|\n",
      "+---------+--------------+\n",
      "|    trees|        260490|\n",
      "|   gaming|        496350|\n",
      "+---------+--------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## data has unequal amount of obs\n",
    "print(df_filtered.groupby('subreddit').agg(count('comment')).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly choose 50000 obs for each class\n",
    "df_gaming = df_filtered.select('comment','subreddit').filter(col(\"subreddit\") == \"gaming\").orderBy(rand()).sample(withReplacement = False, fraction = 1.0, seed = 1000).take(50000)\n",
    "df_trees = df_filtered.select('comment','subreddit').filter(col(\"subreddit\") == \"trees\").orderBy(rand()).sample(withReplacement = False, fraction = 1.0, seed = 1000).take(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gaming = spark.createDataFrame(df_gaming)\n",
    "df_trees = spark.createDataFrame(df_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49373"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of comments in gaming subreddit\n",
    "df_gaming.select('comment').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49136"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of comments in trees subreddit\n",
    "df_trees.select('comment').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## so now data has almost equal number of obs for each class\n",
    "## make union of them for modeling\n",
    "df_union = df_gaming.union(df_trees)\n",
    "\n",
    "## quick check the number of subreddits. it should be only 2\n",
    "df_union.select('subreddit').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union = df_union.withColumn('comment_length', length(df_union['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|subreddit|avg(comment_length)|\n",
      "+---------+-------------------+\n",
      "|    trees|           112.8109|\n",
      "|   gaming|          169.44256|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## length of comments not exactly the same but for this simple modeling that is fine\n",
    "## I address this issue later for more robust modeling\n",
    "df_union.groupby('subreddit').agg({'comment_length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make 0 1 label\n",
    "def labeler(line):\n",
    "    if line == 'gaming':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "labeler_udf = udf(lambda line: labeler(line), IntegerType())\n",
    "\n",
    "df_union = df_union.withColumn('label', labeler_udf('subreddit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+-----+\n",
      "|             comment|subreddit|comment_length|label|\n",
      "+--------------------+---------+--------------+-----+\n",
      "|PA nor Upstate NY...|    trees|            51|    0|\n",
      "|Me and my entwife...|    trees|           147|    0|\n",
      "|http://www.reddit...|    trees|            29|    0|\n",
      "|i bet if we each ...|    trees|            64|    0|\n",
      "|Find some ents to...|    trees|            32|    0|\n",
      "|Happy birthday (a...|    trees|            32|    0|\n",
      "|If you are going ...|    trees|            62|    0|\n",
      "|shove edible down...|    trees|            94|    0|\n",
      "|Some cats are cli...|    trees|            37|    0|\n",
      "|gooners, laughed ...|    trees|            49|    0|\n",
      "|You're 14? Damn.....|    trees|            57|    0|\n",
      "|I'm guessing keep...|    trees|           182|    0|\n",
      "|i downvoted you b...|    trees|           225|    0|\n",
      "|Do you have pictu...|    trees|            82|    0|\n",
      "|You in austin? Le...|    trees|            33|    0|\n",
      "|Been a huge fan s...|    trees|            64|    0|\n",
      "|Happy birthday br...|    trees|            96|    0|\n",
      "|Use hot water or ...|    trees|           154|    0|\n",
      "|This was pretty m...|    trees|           216|    0|\n",
      "|the goon at the e...|    trees|            47|    0|\n",
      "+--------------------+---------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.filter(col('label') == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## randomly order the data\n",
    "df_union = df_union.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------+-----+\n",
      "|             comment|subreddit|comment_length|label|\n",
      "+--------------------+---------+--------------+-----+\n",
      "|Me and a friend s...|   gaming|            39|    1|\n",
      "|Ya but that's why...|    trees|           145|    0|\n",
      "|Tip:  Use freezer...|   gaming|           197|    1|\n",
      "|if someone GIVES ...|    trees|           253|    0|\n",
      "|             REPOST |    trees|             7|    0|\n",
      "|and buddies we sh...|    trees|            94|    0|\n",
      "|...touchè, good sir.|    trees|            20|    0|\n",
      "|It's so trippy ho...|    trees|            93|    0|\n",
      "|By far the best u...|    trees|            55|    0|\n",
      "|          **TOASTY**|   gaming|            10|    1|\n",
      "+--------------------+---------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nltk is needed only for stopwords\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\navruzbek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "\n",
    "## not needed for vader\n",
    "punct_words=list(string.punctuation)\n",
    "def remove_punct(x):\n",
    "    cleaned_sent = [''.join(c for c in s if c not in punct_words) for s in x] \n",
    "    cleaned_sent = [s for s in cleaned_sent if s]\n",
    "    return cleaned_sent\n",
    "  \n",
    "def remove_int(x):\n",
    "    return [a for a in x if a.isdigit() is False]\n",
    "\n",
    "## 14 comes from the notebook download_preprocess\n",
    "def short_words(x):\n",
    "    return [a for a in x if len(a) <= 14]\n",
    "\n",
    "remove_int_udf = udf(lambda line: remove_int(line), ArrayType(StringType()))\n",
    "shorten_words_udf = udf(lambda line: short_words(line), ArrayType(StringType()))\n",
    "remove_punct_udf = udf(lambda line: remove_punct(line), ArrayType(StringType()))\n",
    "token_count_udf = udf(lambda line: len(line), IntegerType())\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='comment', outputCol='comment_tokens')\n",
    "df_tokened = tokenizer.transform(df_union.select('comment','label'))\n",
    "\n",
    "df_tokened = df_tokened.withColumn('comment_tokens_cleaned', remove_int_udf(\"comment_tokens\"))\n",
    "df_tokened = df_tokened.withColumn('comment_tokens_cleaned', shorten_words_udf(\"comment_tokens_cleaned\"))\n",
    "df_tokened = df_tokened.withColumn('comment_tokens_cleaned', remove_punct_udf(\"comment_tokens_cleaned\"))\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol='comment_tokens_cleaned', outputCol='comment_tokens_cleaned_final')\n",
    "df_cleaned = stopword_remover.transform(df_tokened)\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\"comment_cleaned_count_final\", token_count_udf(col('comment_tokens_cleaned_final')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------------------+----------------------------+---------------------------+\n",
      "|             comment|label|      comment_tokens|comment_tokens_cleaned|comment_tokens_cleaned_final|comment_cleaned_count_final|\n",
      "+--------------------+-----+--------------------+----------------------+----------------------------+---------------------------+\n",
      "|Me and a friend s...|    1|[me, and, a, frie...|  [me, and, a, frie...|        [friend, joining,...|                          3|\n",
      "|Ya but that's why...|    0|[ya, but, that's,...|  [ya, but, thats, ...|        [ya, thats, good,...|                         17|\n",
      "|Tip:  Use freezer...|    1|[tip:, , use, fre...|  [tip, use, freeze...|        [tip, use, freeze...|                         19|\n",
      "|if someone GIVES ...|    0|[if, someone, giv...|  [if, someone, giv...|        [someone, gives, ...|                         23|\n",
      "|             REPOST |    0|            [repost]|              [repost]|                    [repost]|                          1|\n",
      "|and buddies we sh...|    0|[and, buddies, we...|  [and, buddies, we...|        [buddies, shall, ...|                         10|\n",
      "|...touchè, good sir.|    0|[...touchè,, good...|   [touchè, good, sir]|         [touchè, good, sir]|                          3|\n",
      "|It's so trippy ho...|    0|[it's, so, trippy...|  [its, so, trippy,...|        [trippy, looks, l...|                          9|\n",
      "|By far the best u...|    0|[by, far, the, be...|  [by, far, the, be...|        [far, best, use, ...|                          6|\n",
      "|          **TOASTY**|    1|        [**toasty**]|              [toasty]|                    [toasty]|                          1|\n",
      "+--------------------+-----+--------------------+----------------------+----------------------------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(inputCol='comment_tokens_cleaned_final', outputCol='features')\n",
    "count_vec_df = count_vec.fit(df_cleaned).transform(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------------------+----------------------------+---------------------------+--------------------+------+\n",
      "|             comment|label|      comment_tokens|comment_tokens_cleaned|comment_tokens_cleaned_final|comment_cleaned_count_final|            features|label_|\n",
      "+--------------------+-----+--------------------+----------------------+----------------------------+---------------------------+--------------------+------+\n",
      "|Me and a friend s...|    1|[me, and, a, frie...|  [me, and, a, frie...|        [friend, joining,...|                          3|(58706,[143,446,5...|     1|\n",
      "|Ya but that's why...|    0|[ya, but, that's,...|  [ya, but, thats, ...|        [ya, thats, good,...|                         17|(58706,[8,17,28,4...|     0|\n",
      "+--------------------+-----+--------------------+----------------------+----------------------------+---------------------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_vec_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = count_vec_df.select(['label','comment_cleaned_count_final','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assemble all features in vec \n",
    "vec_assembler = VectorAssembler(inputCols=['features','comment_cleaned_count_final'], outputCol='feature_vec')\n",
    "model_data = vec_assembler.transform(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data\n",
    "train_df, test_df = model_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the model\n",
    "logistic_reg = LogisticRegression(featuresCol='feature_vec', labelCol='label').fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make predictions\n",
    "evaluation = logistic_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.cache()\n",
    "false_pos = evaluation[(evaluation.label == 0) & (evaluation.prediction == 1)].count()\n",
    "true_pos = evaluation[(evaluation.label == 1) & (evaluation.prediction == 1)].count()\n",
    "false_neg = evaluation[(evaluation.label == 1) & (evaluation.prediction == 0)].count()\n",
    "true_neg = evaluation[(evaluation.label == 0) & (evaluation.prediction == 0)].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the accuracy\n",
    "accuracy = float((true_pos + true_neg) / (evaluation.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8979111356008616\n"
     ]
    }
   ],
   "source": [
    "## pretty high accuracy \n",
    "## after training data on 100K obs\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "subreddit_classification",
  "notebookId": 2376661332894682
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
